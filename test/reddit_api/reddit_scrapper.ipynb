{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bloomberg - Markets': 'https://www.bloomberg.com/markets/rss',\n",
      " 'Business Insider - Markets': 'https://www.businessinsider.com/markets/rss',\n",
      " 'CNBC': 'https://www.cnbc.com/id/10000664/device/rss/rss.html',\n",
      " 'CNBC - Investing': 'https://www.cnbc.com/id/15839069/device/rss/rss.html',\n",
      " 'CNBC - Markets': 'https://www.cnbc.com/id/100003114/device/rss/rss.html',\n",
      " 'Financial Times - Markets': 'https://www.ft.com/markets?format=rss',\n",
      " 'Investing.com': 'https://www.investing.com/rss/news.rss',\n",
      " 'Kiplinger - Personal Finance': 'https://www.kiplinger.com/rss/index.xml',\n",
      " 'Money - Markets': 'https://money.com/markets/feed/',\n",
      " 'Money - Personal Finance': 'https://money.com/personal-finance/feed/',\n",
      " 'Moneycontrol - News': 'https://www.moneycontrol.com/rss/latestnews.xml',\n",
      " 'NerdWallet': 'https://www.nerdwallet.com/blog/feed/',\n",
      " 'NerdWallet - Investing': 'https://www.nerdwallet.com/blog/investing/feed/',\n",
      " 'Reuters - Business News': 'http://feeds.reuters.com/reuters/businessNews',\n",
      " 'Reuters - Markets': 'http://feeds.reuters.com/reuters/marketsNews',\n",
      " 'Reuters - Money': 'http://feeds.reuters.com/news/wealth',\n",
      " 'Seeking Alpha': 'https://seekingalpha.com/feed.xml',\n",
      " 'The Economist - Business': 'http://www.economist.com/feeds/print-sections/77/business.xml',\n",
      " 'The Economist - Finance and Economics': 'http://www.economist.com/feeds/print-sections/79/finance-and-economics.xml',\n",
      " 'The Motley Fool': 'https://www.fool.com/feeds/index.aspx',\n",
      " 'The Wall Street Journal - Markets': 'https://feeds.a.dj.com/rss/RSSMarketsMain.xml',\n",
      " 'Yahoo Finance': 'https://finance.yahoo.com/news/rssindex',\n",
      " 'http://economictimes.indiatimes.com/markets/stocks/rssfeeds/2146842.cms': 'http://economictimes.indiatimes.com/markets/stocks/rssfeeds/2146842.cms',\n",
      " 'https://feeds.content.dowjones.io/public/rss/mw_topstories': 'https://feeds.content.dowjones.io/public/rss/mw_topstories',\n",
      " 'https://sharekhaneducation.com/blog/feed/': 'https://sharekhaneducation.com/blog/feed/',\n",
      " 'https://stockstotrade.com/blog/': 'https://stockstotrade.com/blog/',\n",
      " 'https://tradebrains.in/blog/feed/': 'https://tradebrains.in/blog/feed/',\n",
      " 'https://www.investing.com/rss/news_25.rss': 'https://www.investing.com/rss/news_25.rss'}\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "rss_dict = {\n",
    "            'Yahoo Finance': 'https://finance.yahoo.com/news/rssindex',\n",
    "            'CNBC': 'https://www.cnbc.com/id/10000664/device/rss/rss.html',\n",
    "            'The Economist - Business': 'http://www.economist.com/feeds/print-sections/77/business.xml',\n",
    "            'Reuters - Business News': 'http://feeds.reuters.com/reuters/businessNews',\n",
    "            'Seeking Alpha': 'https://seekingalpha.com/feed.xml',\n",
    "            'Investing.com': 'https://www.investing.com/rss/news.rss',\n",
    "            'Moneycontrol - News': 'https://www.moneycontrol.com/rss/latestnews.xml',\n",
    "            'The Motley Fool': 'https://www.fool.com/feeds/index.aspx',\n",
    "            'Kiplinger - Personal Finance': 'https://www.kiplinger.com/rss/index.xml',\n",
    "            'NerdWallet': 'https://www.nerdwallet.com/blog/feed/',\n",
    "            'The Economist - Finance and Economics': 'http://www.economist.com/feeds/print-sections/79/finance-and-economics.xml',\n",
    "            'Reuters - Money': 'http://feeds.reuters.com/news/wealth',\n",
    "            'Money - Personal Finance': 'https://money.com/personal-finance/feed/',\n",
    "            'NerdWallet - Investing': 'https://www.nerdwallet.com/blog/investing/feed/',\n",
    "            'CNBC - Investing': 'https://www.cnbc.com/id/15839069/device/rss/rss.html',\n",
    "            'Financial Times - Markets': 'https://www.ft.com/markets?format=rss',\n",
    "            'Business Insider - Markets': 'https://www.businessinsider.com/markets/rss',\n",
    "            'Money - Markets': 'https://money.com/markets/feed/',\n",
    "            'CNBC - Markets': 'https://www.cnbc.com/id/100003114/device/rss/rss.html',\n",
    "            'The Wall Street Journal - Markets': 'https://feeds.a.dj.com/rss/RSSMarketsMain.xml',\n",
    "            'Reuters - Markets': 'http://feeds.reuters.com/reuters/marketsNews',\n",
    "            'Bloomberg - Markets': 'https://www.bloomberg.com/markets/rss',\n",
    "            'Financial Times - Markets': 'https://www.ft.com/markets?format=rss',\n",
    "            'Business Insider - Markets': 'https://www.businessinsider.com/markets/rss',\n",
    "            'Money - Markets': 'https://money.com/markets/feed/',\n",
    "            'CNBC - Markets': 'https://www.cnbc.com/id/100003114/device/rss/rss.html',\n",
    "            'https://feeds.content.dowjones.io/public/rss/mw_topstories': 'https://feeds.content.dowjones.io/public/rss/mw_topstories',\n",
    "            'https://www.investing.com/rss/news_25.rss': 'https://www.investing.com/rss/news_25.rss',\n",
    "            'https://stockstotrade.com/blog/': 'https://stockstotrade.com/blog/',\n",
    "            'http://economictimes.indiatimes.com/markets/stocks/rssfeeds/2146842.cms': 'http://economictimes.indiatimes.com/markets/stocks/rssfeeds/2146842.cms',\n",
    "            'https://sharekhaneducation.com/blog/feed/': 'https://sharekhaneducation.com/blog/feed/',\n",
    "            'https://tradebrains.in/blog/feed/': 'https://tradebrains.in/blog/feed/'\n",
    "}\n",
    "\n",
    "succeded_dict = {}\n",
    "\n",
    "for key, value in rss_dict.items():\n",
    "    try:\n",
    "        feed = feedparser.parse(value)\n",
    "        succeded_dict[key] = value\n",
    "    except:\n",
    "        print(f\"Failed to parse {key}\")\n",
    "        pass\n",
    "from pprint import pprint   \n",
    "pprint(succeded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Single Subreddit (Fetch Posts Only) ===\n",
      "Fetching page 1 from r/NVDA_Stock (top), time_filter=day ...\n",
      "Fetched 3 posts from r/NVDA_Stock\n",
      " - $53 of NVDA in 2001 (ID: 1hr6wv8)\n",
      " - From Bulls to Bears: Surviving 2025 with My NVDA Reddit Crew (ID: 1hqooaa)\n",
      " - want to pull the trigger, but... (ID: 1hqmt3w)\n",
      "\n",
      "=== Test 2: Multiple Subreddits (Fetch Posts + Comments) ===\n",
      "Fetching page 1 from r/nvidia (top), time_filter=day ...\n",
      "Fetching page 1 from r/NVDA_Stock (top), time_filter=day ...\n",
      "Fetching comments for post 1hr03sm in r/nvidia ...\n",
      "Fetching comments for post 1hr27i5 in r/nvidia ...\n",
      "Fetching comments for post 1hr6wv8 in r/NVDA_Stock ...\n",
      "Fetching comments for post 1hqooaa in r/NVDA_Stock ...\n",
      "\n",
      "r/nvidia: 2 posts fetched.\n",
      "  Title: NVIDIA GeForce RTX 5080 reportedly launches January 21st - VideoCardz.com => 59 comments\n",
      "  Title: Nintendo Switch 2 motherboard with NVIDIA SoC leaks out - VideoCardz.com => 24 comments\n",
      "\n",
      "r/NVDA_Stock: 2 posts fetched.\n",
      "  Title: $53 of NVDA in 2001 => 49 comments\n",
      "  Title: From Bulls to Bears: Surviving 2025 with My NVDA Reddit Crew => 7 comments\n",
      "\n",
      "--- Combined Text (Truncated for Demo) ---\n",
      "=== Post #1 ===\n",
      "Title: NVIDIA GeForce RTX 5080 reportedly launches January 21st - VideoCardz.com\n",
      "Body: \n",
      "--- Comments (Up to level 2) ---\n",
      "  - (level 1) Comment by Ziggyvertang: Just quietly waiting here with my 2070super waiting to see what upgrade options I got to me.\n",
      "    - (level 2) Comment by Mookmookmook: Same. Feels like itâ€™s time.\n",
      "\n",
      "Not liking the sound of them being stingy with the VRAM though.Â \n",
      "    - (level 2) Comment by leahcim2019: 1070 here ðŸ¤£ any upgrade will be good for me lol\n",
      "    - (level 2) Comment by Intelligent_Toe684: Iâ€™m still here with my 1070 lol think Iâ€™m gonna pass on this 50 line and settle for something like a 4070 Super\n",
      "    - (level 2) Comment by Archangel959: Right there with you. Still rocking a 1080 and waiting to see where the chips fall.\n",
      "  - (level 1) Comment by Lo_jak: The way this product stack is looking kinda signals that there is going to be a 5080ti that will sit slap bang between the 5080 and the 5090..... that will be the true \"5080\".\n",
      "\n",
      "What we are se ... [TRUNCATED]\n",
      "\n",
      "=== Test 3: Single Subreddit Search ===\n",
      "Found 2 posts in r/stocks about 'NVDA'\n",
      "   1. Portfolio in 2025 (score: 47)\n",
      "   2. The AI Revolution: Top 10 Tech Stocks Poised for Success in 2025 (score: 0)\n",
      "\n",
      "--- Searched Posts Combined Text (Truncated) ---\n",
      "=== Post #1 ===\n",
      "Title: Portfolio in 2025\n",
      "Body: As 2025 looms around the corner, looking at my portfolio to see if I am on the right path or needs to make some adjustments.  Any critique?\n",
      "\n",
      "GOOG, \n",
      "AMZN, \n",
      "T, \n",
      "NVDA, \n",
      "O, \n",
      "RKLB, \n",
      "SHW, \n",
      "KO, \n",
      "WBD, \n",
      "CLM, \n",
      "SCHD, \n",
      "VOO\n",
      "\n",
      "Aside from this taxable account, I also own the following in my tax deferred:\n",
      "\n",
      "VFAIX, VTSAX, JEPI, VT, VYM, VTI\n",
      "\n",
      "--- No Comments (or Comments Suppressed) ---\n",
      "\n",
      "\n",
      "=== Post #2 ===\n",
      "Title: The AI Revolution: Top 10 Tech Stocks Poised for Success in 2025\n",
      "Body: Wedbush named its top 10 pick for 2025    \n",
      "   \n",
      "1. Nvidia $NVDA \n",
      "\n",
      "2. Apple $AAPL \n",
      "\n",
      "3. Palantir $PLTR \n",
      "\n",
      "4. Tesla $TSLA \n",
      "\n",
      "5. Snowflake $SNOW \n",
      "\n",
      "6. Salesforce $CRM \n",
      "\n",
      "7. MongoDB $MDB \n",
      "\n",
      "8. Microsoft $MSFT \n",
      "\n",
      "9. Alphabet $GOOGL \n",
      "\n",
      "10. Pegasystems $PEGA\n",
      "--- No Comments (or Comments Suppressed) ---\n",
      "\n",
      " ... [TRUNCATED]\n",
      "\n",
      "=== Test 4: Global Search ===\n",
      "Fetched 2 global results about 'NVIDIA'\n",
      "\n",
      "=== Test 5: Global Search, Fetch Comments, Flatten to Text (max_comment_level=2) ===\n",
      "Performing global search for 'NVDA', limit=2, sort=relevance, time_filter=day\n",
      "Fetching comments for post 1hr6wv8 in r/NVDA_Stock ...\n",
      "Fetching comments for post 1hr7rs2 in r/wallstreetbets ...\n",
      "\n",
      "--- Final Combined Text (Truncated for Demo) ---\n",
      "=== Post #1 ===\n",
      "Title: $53 of NVDA in 2001\n",
      "Body: Since my wife is tired of hearing about itâ€¦ I thought Iâ€™d share the lucky bet I made on a local tech company a friendâ€™s dad recommended. I figured $50 + the trade fee is about what I could afford. I think I bought 10 Bay Area micro-caps total, hoping one would outperform. 9 went bust, but NVDA is still going strong. ðŸ™‚ \n",
      "--- Comments (Up to level 2) ---\n",
      "  - (level 1) Comment by ProfessorAkaliOnYT: that is the most beautiful gain I've ever seen\n",
      "    - (level 2) Comment by Wladyslaw_Spindlesha: Dumb luck! But about time I win one. ðŸ˜‚\n",
      "  - (level 1) Comment by ExplorerWildfire: 24 years of patience ðŸ˜‚\n",
      "    - (level 2) Comment by Wladyslaw_Spindlesha: Right?! ðŸ™‚\n",
      "    - (level 2) Comment by b1ack1323: I mean $53 is a toss, are you really gonna sell that if your portfolio is like $100k or more in other investments?\n",
      "  - (level 1) Comment by riri101628: I should've invested my pocket money when I was in kindergartenðŸ˜­\n",
      "    - (level 2) Comment by silent-dan ... [TRUNCATED]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import sys\n",
    "from typing import Union, List, Dict, Any, Optional\n",
    "\n",
    "\n",
    "class RedditScraper:\n",
    "    \"\"\"\n",
    "    A simple class to fetch subreddit posts and comments from Reddit's\n",
    "    (mostly) public JSON endpoints. Suitable for small-scale or personal\n",
    "    usage. For production or large-scale usage, switch to an OAuth-based\n",
    "    approach (e.g., PRAW).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_agent: str = \"MyRedditApp/1.0 (by u/my_username)\",\n",
    "        max_pages: int = 2,\n",
    "        sleep_time: float = 2.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RedditScraper.\n",
    "\n",
    "        :param user_agent: Custom User-Agent string to identify your client.\n",
    "        :param max_pages: Default number of 'pages' to paginate for posts.\n",
    "        :param sleep_time: Pause (in seconds) between page fetches to avoid rate-limits.\n",
    "        \"\"\"\n",
    "        self.user_agent = user_agent\n",
    "        self.headers = {\"User-Agent\": self.user_agent}\n",
    "        self.max_pages = max_pages\n",
    "        self.sleep_time = sleep_time\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # (A) Fetching Subreddit Posts\n",
    "    # ---------------------------------------------------------------------\n",
    "    def fetch_subreddit_posts(\n",
    "        self,\n",
    "        subreddits: Union[str, List[str]],\n",
    "        sort: str = \"new\",\n",
    "        limit: int = 25,\n",
    "        max_pages: Optional[int] = None,\n",
    "        time_filter: Optional[str] = None\n",
    "    ) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Fetch posts from one or more subreddits. Supports pagination and time filtering.\n",
    "\n",
    "        :param subreddits: A single subreddit (string) or list of subreddit names.\n",
    "        :param sort: Sorting criterion (e.g., \"new\", \"hot\", \"top\", \"rising\", \"controversial\").\n",
    "        :param limit: Number of posts to fetch per request (max ~100).\n",
    "        :param max_pages: How many pages to fetch. Defaults to self.max_pages if None.\n",
    "        :param time_filter: Time filter (valid if sort=\"top\" or \"controversial\"), \n",
    "                            e.g., \"day\", \"week\", \"month\", \"year\".\n",
    "        :return: Dictionary where key=subreddit, value=list of post data dicts.\n",
    "        \"\"\"\n",
    "        if isinstance(subreddits, str):\n",
    "            subreddits = [subreddits]  # Wrap in a list for uniform processing\n",
    "\n",
    "        if max_pages is None:\n",
    "            max_pages = self.max_pages\n",
    "\n",
    "        results = {}\n",
    "        for subreddit in subreddits:\n",
    "            all_posts = []\n",
    "            after = None\n",
    "\n",
    "            for page in range(max_pages):\n",
    "                print(f\"Fetching page {page + 1} from r/{subreddit} ({sort}), time_filter={time_filter} ...\")\n",
    "                page_data = self._fetch_subreddit_page(\n",
    "                    subreddit=subreddit,\n",
    "                    sort=sort,\n",
    "                    limit=limit,\n",
    "                    after=after,\n",
    "                    time_filter=time_filter\n",
    "                )\n",
    "\n",
    "                if not page_data:\n",
    "                    # Something went wrong, or no posts returned\n",
    "                    break\n",
    "\n",
    "                all_posts.extend(page_data[\"posts\"])\n",
    "                after = page_data[\"after\"]\n",
    "\n",
    "                if not after:\n",
    "                    # No more pagination\n",
    "                    break\n",
    "\n",
    "                # Sleep to respect rate limits\n",
    "                time.sleep(self.sleep_time)\n",
    "\n",
    "            results[subreddit] = all_posts\n",
    "\n",
    "        return results\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # (B) Fetching Comments for a Specific Post\n",
    "    # ---------------------------------------------------------------------\n",
    "    def fetch_comments_for_post(\n",
    "        self,\n",
    "        subreddit: str,\n",
    "        post_id: str\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Fetch comments for a specific post in a subreddit.\n",
    "        (Does not apply comment depth limitation here; see 'posts_and_comments_to_text'\n",
    "         or your own logic to limit levels if needed.)\n",
    "\n",
    "        :param subreddit: Subreddit name (e.g., \"python\").\n",
    "        :param post_id: The base-36 ID of the post (e.g. \"abc123\").\n",
    "        :return: List of parsed comments, including nested replies.\n",
    "                 Note: Includes *all* comment levels. You may filter later if desired.\n",
    "        \"\"\"\n",
    "        url = f\"https://www.reddit.com/r/{subreddit}/comments/{post_id}.json\"\n",
    "        print(f\"Fetching comments for post {post_id} in r/{subreddit} ...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            return []\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: HTTP {response.status_code} for {url}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"Error parsing JSON response.\")\n",
    "            return []\n",
    "\n",
    "        # data[0] is post info, data[1] is the comment listing\n",
    "        if len(data) < 2:\n",
    "            return []\n",
    "\n",
    "        comments_root = data[1]\n",
    "        comments_list = comments_root.get(\"data\", {}).get(\"children\", [])\n",
    "        parsed_comments = []\n",
    "\n",
    "        for comment in comments_list:\n",
    "            if comment.get(\"kind\") == \"t1\":  # 't1' = comment\n",
    "                parsed_comments.append(self._parse_comment(comment))\n",
    "\n",
    "        return parsed_comments\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # (C) Fetching Both Posts and Comments (Subreddit-level)\n",
    "    # ---------------------------------------------------------------------\n",
    "    def fetch_posts_and_comments(\n",
    "        self,\n",
    "        subreddits: Union[str, List[str]],\n",
    "        sort: str = \"new\",\n",
    "        limit: int = 25,\n",
    "        max_pages: Optional[int] = None,\n",
    "        time_filter: Optional[str] = None\n",
    "    ) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Convenience method to fetch posts from one or multiple subreddits,\n",
    "        and then fetch comments for each post. Returns a fully nested structure.\n",
    "\n",
    "        :param subreddits: A single subreddit (string) or list of subreddit names.\n",
    "        :param sort: Sorting criterion (e.g., \"new\", \"hot\", \"top\", \"rising\", \"controversial\").\n",
    "        :param limit: Number of posts to fetch per request.\n",
    "        :param max_pages: How many pages to fetch (for each subreddit).\n",
    "        :param time_filter: Time filter (valid if sort=\"top\" or \"controversial\"), e.g., \"day\", \"week\", etc.\n",
    "        :return: {\n",
    "            \"<subreddit>\": [\n",
    "                {\n",
    "                    \"id\": <post_id>,\n",
    "                    \"title\": \"...\",\n",
    "                    \"selftext\": \"...\",\n",
    "                    \"comments\": [ ... list of comments ... ],\n",
    "                    ...\n",
    "                },\n",
    "                ...\n",
    "            ],\n",
    "            ...\n",
    "        }\n",
    "        \"\"\"\n",
    "        posts_dict = self.fetch_subreddit_posts(\n",
    "            subreddits=subreddits,\n",
    "            sort=sort,\n",
    "            limit=limit,\n",
    "            max_pages=max_pages,\n",
    "            time_filter=time_filter\n",
    "        )\n",
    "\n",
    "        # Now, fetch comments for each post\n",
    "        for subreddit, posts_list in posts_dict.items():\n",
    "            for post_data in posts_list:\n",
    "                post_id = post_data.get(\"id\")\n",
    "                if post_id:\n",
    "                    comments = self.fetch_comments_for_post(subreddit, post_id)\n",
    "                    post_data[\"comments\"] = comments\n",
    "                else:\n",
    "                    post_data[\"comments\"] = []\n",
    "\n",
    "        return posts_dict\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # (D) Searching Within a Single Subreddit\n",
    "    # ---------------------------------------------------------------------\n",
    "    def search_subreddit_posts(\n",
    "        self,\n",
    "        subreddit: str,\n",
    "        query: str,\n",
    "        sort: str = \"relevance\",\n",
    "        time_filter: Optional[str] = None,\n",
    "        limit: int = 25\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search a specific subreddit for posts matching a given query.\n",
    "\n",
    "        :param subreddit: The name of the subreddit (e.g., \"python\").\n",
    "        :param query: The search query string (e.g., \"web scraping\").\n",
    "        :param sort: One of ['relevance', 'hot', 'top', 'new', 'comments'].\n",
    "        :param time_filter: Filter posts by time ('hour', 'day', 'week', 'month', 'year', 'all').\n",
    "        :param limit: Number of posts to fetch in one request (up to ~100).\n",
    "        :return: A list of post data dicts.\n",
    "        \"\"\"\n",
    "        base_url = f\"https://www.reddit.com/r/{subreddit}/search.json\"\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"restrict_sr\": \"1\",   # search only in this subreddit\n",
    "            \"sort\": sort,\n",
    "            \"limit\": limit\n",
    "        }\n",
    "        if time_filter:\n",
    "            params[\"t\"] = time_filter\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, headers=self.headers, params=params)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            return []\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: HTTP {response.status_code} for {base_url}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"Error parsing JSON.\")\n",
    "            return []\n",
    "\n",
    "        posts = []\n",
    "        children = data.get(\"data\", {}).get(\"children\", [])\n",
    "        for child in children:\n",
    "            if child.get(\"kind\") == \"t3\":\n",
    "                post_info = child.get(\"data\", {})\n",
    "                posts.append(post_info)\n",
    "\n",
    "        return posts\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # (E) Global (All-Reddit) Keyword Search\n",
    "    # ---------------------------------------------------------------------\n",
    "    def global_reddit_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        limit: int = 25,\n",
    "        sort: str = \"relevance\",\n",
    "        time_filter: str = \"all\",\n",
    "        after: Optional[str] = None\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Perform a keyword search across ALL of Reddit (unauthenticated).\n",
    "\n",
    "        :param query: The search query (string).\n",
    "        :param limit: Number of search results to fetch (max ~100 per request).\n",
    "        :param sort: Sorting criterion: \"relevance\", \"hot\", \"top\", \"new\", \"comments\".\n",
    "        :param time_filter: Time filter: \"hour\", \"day\", \"week\", \"month\", \"year\", \"all\".\n",
    "        :param after: Pagination token for the next set of results.\n",
    "        :return: A dict with 'results' (list of posts) and 'after' (token to get next page),\n",
    "                 or None if an error occurs.\n",
    "        \"\"\"\n",
    "        base_url = \"https://www.reddit.com/search.json\"\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"limit\": limit,\n",
    "            \"sort\": sort,\n",
    "            \"t\": time_filter\n",
    "        }\n",
    "        if after:\n",
    "            params[\"after\"] = after\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, headers=self.headers, params=params)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            return None\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: HTTP {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"Error parsing JSON.\")\n",
    "            return None\n",
    "\n",
    "        children = data.get(\"data\", {}).get(\"children\", [])\n",
    "        results = []\n",
    "\n",
    "        for child in children:\n",
    "            if child.get(\"kind\") == \"t3\":  # 't3' means it's a post\n",
    "                post_data = child.get(\"data\", {})\n",
    "                results.append(post_data)\n",
    "\n",
    "        after_token = data.get(\"data\", {}).get(\"after\", None)\n",
    "        return {\"results\": results, \"after\": after_token}\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # (F) Search by Keywords, Fetch Comments, Return in Single Structure\n",
    "    # ---------------------------------------------------------------------\n",
    "    def search_keywords_and_get_posts_comments(\n",
    "        self,\n",
    "        query: str,\n",
    "        limit: int = 25,\n",
    "        sort: str = \"relevance\",\n",
    "        time_filter: str = \"all\"\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Global search by keywords, then fetch comments for each post. \n",
    "        Returns a list of post dicts, each including \"comments\".\n",
    "\n",
    "        :param query: The search query.\n",
    "        :param limit: Max results (per fetch). For large numbers, consider paging.\n",
    "        :param sort: \"relevance\", \"hot\", \"top\", \"new\", \"comments\".\n",
    "        :param time_filter: \"hour\", \"day\", \"week\", \"month\", \"year\", \"all\".\n",
    "        :return: A list of posts, each with a nested \"comments\" field.\n",
    "        \"\"\"\n",
    "        print(f\"Performing global search for '{query}', limit={limit}, sort={sort}, time_filter={time_filter}\")\n",
    "        # 1) Perform a single global search call (no pagination here; you can extend if needed).\n",
    "        search_result = self.global_reddit_search(\n",
    "            query=query,\n",
    "            limit=limit,\n",
    "            sort=sort,\n",
    "            time_filter=time_filter\n",
    "        )\n",
    "        if not search_result:\n",
    "            return []\n",
    "\n",
    "        posts = search_result.get(\"results\", [])\n",
    "        # 2) For each post, fetch comments by subreddit/post_id\n",
    "        for post in posts:\n",
    "            subr = post.get(\"subreddit\")\n",
    "            pid = post.get(\"id\")\n",
    "            if subr and pid:\n",
    "                # Fetch comments for each post\n",
    "                comments = self.fetch_comments_for_post(subr, pid)\n",
    "                post[\"comments\"] = comments\n",
    "            else:\n",
    "                post[\"comments\"] = []\n",
    "\n",
    "        return posts\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # (G) Convert Posts (and Nested Comments) to a Single Text String\n",
    "    # ---------------------------------------------------------------------\n",
    "    def posts_and_comments_to_text(\n",
    "        self,\n",
    "        posts_data: List[Dict[str, Any]],\n",
    "        max_comment_level: int = 999\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Flatten/serialize a list of post dicts (each with nested comments)\n",
    "        into a single large text string for easier saving or processing.\n",
    "\n",
    "        :param posts_data: A list of post dicts, each with optional \"comments\" array.\n",
    "        :param max_comment_level: The maximum depth of comment nesting to include.\n",
    "                                  0 means no comments, 1 = only top-level, etc.\n",
    "        :return: A single text string containing posts & comments.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        for i, post in enumerate(posts_data, start=1):\n",
    "            title = post.get(\"title\", \"[No Title]\")\n",
    "            selftext = post.get(\"selftext\", \"\")\n",
    "            lines.append(f\"=== Post #{i} ===\")\n",
    "            lines.append(f\"Title: {title}\")\n",
    "            lines.append(f\"Body: {selftext}\")\n",
    "\n",
    "            # Convert comments to text, respecting max_comment_level\n",
    "            comments_list = post.get(\"comments\", [])\n",
    "            if max_comment_level > 0 and comments_list:\n",
    "                lines.append(f\"--- Comments (Up to level {max_comment_level}) ---\")\n",
    "                for comment_str in self._flatten_comment_text(\n",
    "                        comments_list, \n",
    "                        current_level=1, \n",
    "                        max_level=max_comment_level):\n",
    "                    lines.append(comment_str)\n",
    "            else:\n",
    "                lines.append(\"--- No Comments (or Comments Suppressed) ---\")\n",
    "\n",
    "            lines.append(\"\\n\")  # Blank line after each post\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def _flatten_comment_text(\n",
    "        self,\n",
    "        comments: List[Dict[str, Any]],\n",
    "        current_level: int,\n",
    "        max_level: int\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Recursively flatten comment trees into a list of strings, respecting max_level.\n",
    "        Skips comments where author=\"[deleted]\" or body=\"[removed]\".\n",
    "\n",
    "        :param comments: List of comment dicts (each may have 'replies').\n",
    "        :param current_level: Current nesting level in the comment tree.\n",
    "        :param max_level: The maximum nesting level to include.\n",
    "        :return: A list of comment lines (strings).\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        if current_level > max_level:\n",
    "            return lines  # stop if we've exceeded desired depth\n",
    "\n",
    "        indent = \"  \" * current_level  # Indentation for readability\n",
    "        for c in comments:\n",
    "            body = c.get(\"body\", \"[No text]\")\n",
    "            author = c.get(\"author\", \"[unknown]\")\n",
    "            if author == \"[deleted]\" or body == \"[removed]\":\n",
    "                # Skip comments that are effectively removed or deleted\n",
    "                continue\n",
    "\n",
    "            # Include this comment\n",
    "            lines.append(f\"{indent}- (level {current_level}) Comment by {author}: {body}\")\n",
    "\n",
    "            replies = c.get(\"replies\", [])\n",
    "            if replies:\n",
    "                # Recurse deeper if we haven't exceeded max_level\n",
    "                lines.extend(\n",
    "                    self._flatten_comment_text(\n",
    "                        replies, \n",
    "                        current_level=current_level + 1, \n",
    "                        max_level=max_level\n",
    "                    )\n",
    "                )\n",
    "        return lines\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    #   Internal Helper Methods for Subreddit Pages & Parsing Comments\n",
    "    # ---------------------------------------------------------------------\n",
    "    def _fetch_subreddit_page(\n",
    "        self,\n",
    "        subreddit: str,\n",
    "        sort: str,\n",
    "        limit: int,\n",
    "        after: Optional[str],\n",
    "        time_filter: Optional[str]\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Fetch a single \"page\" (one HTTP request) of subreddit posts.\n",
    "        Returns None if an error occurs or no data.\n",
    "\n",
    "        :param subreddit: Name of the subreddit (e.g., \"python\").\n",
    "        :param sort: Sorting criterion (e.g., \"new\", \"hot\", \"top\", \"rising\").\n",
    "        :param limit: Number of posts per request (max ~100).\n",
    "        :param after: Pagination token for next page.\n",
    "        :param time_filter: Time filter if sort=\"top\" or \"controversial\" (e.g. \"day\", \"week\", \"month\").\n",
    "        \"\"\"\n",
    "        base_url = f\"https://www.reddit.com/r/{subreddit}/{sort}.json\"\n",
    "        params = {\"limit\": limit}\n",
    "        if after:\n",
    "            params[\"after\"] = after\n",
    "\n",
    "        if time_filter:\n",
    "            params[\"t\"] = time_filter\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, headers=self.headers, params=params)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            return None\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: HTTP {response.status_code} for {base_url}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except ValueError:\n",
    "            print(\"Error parsing JSON response.\")\n",
    "            return None\n",
    "\n",
    "        children = data.get(\"data\", {}).get(\"children\", [])\n",
    "        posts = []\n",
    "\n",
    "        for child in children:\n",
    "            if child.get(\"kind\") == \"t3\":  # 't3' = post\n",
    "                post_info = child.get(\"data\", {})\n",
    "                posts.append(post_info)\n",
    "\n",
    "        after_token = data.get(\"data\", {}).get(\"after\", None)\n",
    "        return {\n",
    "            \"posts\": posts,\n",
    "            \"after\": after_token\n",
    "        }\n",
    "\n",
    "    def _parse_comment(self, comment_obj: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Recursively parse a comment (and its replies) into a dictionary.\n",
    "        (Does not skip \"[deleted]\" or \"[removed]\" here; skipping logic \n",
    "         is applied later in _flatten_comment_text.)\n",
    "        \"\"\"\n",
    "        comment_data = comment_obj.get(\"data\", {})\n",
    "        comment_id = comment_data.get(\"id\")\n",
    "        comment_author = comment_data.get(\"author\")\n",
    "        comment_body = comment_data.get(\"body\", \"\")\n",
    "        created_utc = comment_data.get(\"created_utc\", 0)\n",
    "\n",
    "        # Recursively parse any replies\n",
    "        replies_obj = comment_data.get(\"replies\")\n",
    "        nested_comments = []\n",
    "        if isinstance(replies_obj, dict):  # sometimes 'replies' can be an empty string\n",
    "            children = replies_obj.get(\"data\", {}).get(\"children\", [])\n",
    "            for child in children:\n",
    "                if child.get(\"kind\") == \"t1\":\n",
    "                    nested_comments.append(self._parse_comment(child))\n",
    "\n",
    "        return {\n",
    "            \"comment_id\": comment_id,\n",
    "            \"author\": comment_author,\n",
    "            \"body\": comment_body,\n",
    "            \"created_utc\": created_utc,\n",
    "            \"replies\": nested_comments,\n",
    "        }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Example Usage / Test Cases\n",
    "# ---------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        scraper = RedditScraper(\n",
    "            user_agent=\"MyRedditApp/1.0 (by u/my_username)\",\n",
    "            max_pages=1,\n",
    "            sleep_time=1.5\n",
    "        )\n",
    "\n",
    "        ##########\n",
    "        print(\"=== Test 1: Single Subreddit (Fetch Posts Only) ===\")\n",
    "        single_sub_data = scraper.fetch_subreddit_posts(\n",
    "            subreddits=\"NVDA_Stock\",\n",
    "            sort=\"top\",\n",
    "            time_filter=\"day\",\n",
    "            limit=3,\n",
    "        )\n",
    "        print(f\"Fetched {len(single_sub_data['NVDA_Stock'])} posts from r/NVDA_Stock\")\n",
    "        for post in single_sub_data[\"NVDA_Stock\"]:\n",
    "            print(f\" - {post.get('title')} (ID: {post.get('id')})\")\n",
    "        \n",
    "        ##########\n",
    "        print(\"\\n=== Test 2: Multiple Subreddits (Fetch Posts + Comments) ===\")\n",
    "        multi_sub_data = scraper.fetch_posts_and_comments(\n",
    "            subreddits=[\"nvidia\", \"NVDA_Stock\"],\n",
    "            sort=\"top\",\n",
    "            time_filter=\"day\",\n",
    "            limit=2,\n",
    "            max_pages=1  # just 1 page each for brevity\n",
    "        )\n",
    "        for subr, posts in multi_sub_data.items():\n",
    "            print(f\"\\nr/{subr}: {len(posts)} posts fetched.\")\n",
    "            for p in posts:\n",
    "                print(f\"  Title: {p.get('title')} => {len(p.get('comments', []))} comments\")\n",
    "\n",
    "        # ------------------------------------------\n",
    "        # CORRECTION:\n",
    "        # To generate combined text for ALL subreddits, first collect all posts in one list.\n",
    "        # ------------------------------------------\n",
    "        all_posts = []\n",
    "        for subr, posts in multi_sub_data.items():\n",
    "            all_posts.extend(posts)\n",
    "\n",
    "        combined_text = scraper.posts_and_comments_to_text(all_posts, max_comment_level=2)\n",
    "        print(\"\\n--- Combined Text (Truncated for Demo) ---\")\n",
    "        print(combined_text[:1000], \"... [TRUNCATED]\")\n",
    "\n",
    "\n",
    "        ##########\n",
    "        print(\"\\n=== Test 3: Single Subreddit Search ===\")\n",
    "        srch_results = scraper.search_subreddit_posts(\n",
    "            subreddit=\"stocks\",\n",
    "            query=\"NVDA\",\n",
    "            sort=\"top\",\n",
    "            time_filter=\"week\",\n",
    "            limit=2\n",
    "        )\n",
    "        print(f\"Found {len(srch_results)} posts in r/stocks about 'NVDA'\")\n",
    "        for i, post in enumerate(srch_results, start=1):\n",
    "            print(f\"   {i}. {post.get('title')} (score: {post.get('score')})\")\n",
    "\n",
    "        # You can flatten these search results into a single text as well:\n",
    "        srch_text = scraper.posts_and_comments_to_text(srch_results, max_comment_level=2)\n",
    "        print(\"\\n--- Searched Posts Combined Text (Truncated) ---\")\n",
    "        print(srch_text[:1000], \"... [TRUNCATED]\")      \n",
    "\n",
    "\n",
    "        ##########\n",
    "        print(\"\\n=== Test 4: Global Search ===\")\n",
    "        global_res = scraper.global_reddit_search(\n",
    "            query=\"NVIDIA\",\n",
    "            limit=2,\n",
    "            sort=\"relevance\",\n",
    "            time_filter=\"day\"\n",
    "        )\n",
    "        if global_res:\n",
    "            print(f\"Fetched {len(global_res['results'])} global results about 'NVIDIA'\")\n",
    "\n",
    "        print(\"\\n=== Test 5: Global Search, Fetch Comments, Flatten to Text (max_comment_level=2) ===\")\n",
    "        posts_with_comments = scraper.search_keywords_and_get_posts_comments(\n",
    "            query=\"NVDA\",\n",
    "            limit=2,\n",
    "            sort=\"relevance\",\n",
    "            time_filter=\"day\"\n",
    "        )\n",
    "        \n",
    "        # Now parse into a single text string, limiting to 2 levels of nested comments.\n",
    "        final_text = scraper.posts_and_comments_to_text(posts_with_comments, max_comment_level=2)\n",
    "        print(\"\\n--- Final Combined Text (Truncated for Demo) ---\")\n",
    "        print(final_text[:1000], \"... [TRUNCATED]\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScript interrupted by user.\")\n",
    "        sys.exit(0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
